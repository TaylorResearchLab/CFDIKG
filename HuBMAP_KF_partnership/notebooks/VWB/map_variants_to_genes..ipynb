{
  "metadata": {
    "name": "map_variants_to_genes",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pandas as pd\npd.set_option(\u0027display.max_columns\u0027, None)\nimport numpy as np\nimport glob\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport io\nimport requests\nimport os\nfrom pyspark import SparkFiles\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col,collect_list, concat, lit, when, monotonically_increasing_id, concat_ws \nfrom pyspark.sql.types import StringType"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####  .12%  of the indels (9700 out of 8mil) are passing CADD score \u003e\u003d 20 filtering (gnomad v3 probably doesnt contain most of the indels in kids first? need to check)\n#### .46% of the SNPs are (161k out of 35mil) are passing CADD score \u003e\u003d 20 filtering\n\n\n#### star format for deletions in kids first -- replace star w/ first base of ref like the other data has. --ask jeremy\n\n#### 20% \u003e kf, filter by common gnomad\n\n#### Do CADD filtering with eQTLs?    just get CADD scores\n\n#### CADD score \u003e\u003d 20 filtering removes a third of the HIGH Impact variants.\n### keep them and just get CADD score\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\naws s3 cp cavatica/projects/taylordm/taylor-urbs-r03-kf-cardiac/variant_annotations.parquet s3://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/variant_annotations.parquet  --recursive\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# \nchd_BS_scored_vars \u003d spark.read.parquet(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/chd_scored_variant_files/CHD_BS_0302Y3N5_scored_variants.parquet\")\nprint(chd_BS_scored_vars.count())\nchd_BS_scored_vars.show(1)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nno_genes \u003d chd_BS_scored_vars.where(col(\u0027symbol\u0027) \u003d\u003d \u0027\u0027)\nhas_genes \u003d chd_BS_scored_vars.where(~(col(\u0027symbol\u0027) \u003d\u003d \u0027\u0027))\nprint(no_genes.count())\nno_genes.show(1)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\u0027\u0027\u0027t_csq \u003d spark \\\n    .table(\u0027consequences\u0027) \\\n    .withColumnRenamed(\u0027name\u0027, \u0027rsID\u0027)\\\n    .drop(\u0027variant_class\u0027)\\\n    .where( (F.array_contains( F.col(\u0027study_ids\u0027),\u0027SD_PREASA7S\u0027 )) \u0026 (F.col(\u0027original_canonical\u0027) \u003d\u003d \u0027true\u0027)) # \u0026 (F.col(\u0027study_ids\u0027).isin(study_id_list))\n    \nprint(t_csq.count())   # 205504922, 41879789 w/ original_canonical\u003dtrue\n\nt_csq_select \u003d t_csq.select([\u0027chromosome\u0027,\u0027start\u0027,\u0027end\u0027, \u0027reference\u0027,\u0027alternate\u0027,\u0027rsID\u0027,\u0027impact\u0027,\u0027symbol\u0027\n                                ,\u0027biotype\u0027,\u0027hgvsc\u0027,\u0027hgvsg\u0027,\u0027feature_type\u0027,\u0027strand\u0027,\u0027consequences\u0027]) \u0027\u0027\u0027\n                                                \nt_csq_select \u003d spark.read.parquet(\u0027s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/variant_annotations.parquet\u0027)  \nt_csq_select \u003d t_csq_select.withColumn(\u0027unique_variant_id\u0027,concat(col(\"chromosome\"), lit(\":\"),col(\"start\"),\n                                                lit(\u0027:\u0027),col(\u0027reference\u0027),lit(\u0027:\u0027),col(\u0027alternate\u0027))) "
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nj \u003d t_csq_select.select(\u0027unique_variant_id\u0027,\u0027rsID\u0027,\u0027symbol\u0027).join(F.broadcast(no_genes.drop(\u0027symbol\u0027,\u0027rsID\u0027)),[\u0027unique_variant_id\u0027],\u0027right\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(j.count())\nj.show(2)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nju \u003d j.select(\u0027unique_variant_id\u0027).toPandas()[\u0027unique_variant_id\u0027].values\nprint(len(ju))\nprint(len(set(ju)))"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nno_genes_vars \u003d no_genes.select(\u0027unique_variant_id\u0027).toPandas()[\u0027unique_variant_id\u0027].values\nprint(len(no_genes_vars))\nprint(len(set(no_genes_vars)))"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nCounter(j.select(\u0027symbol\u0027).toPandas()[\u0027symbol\u0027].values).most_common()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nCounter(j.drop_duplicates([\u0027unique_variant_id\u0027]).select(\u0027symbol\u0027).toPandas()[\u0027symbol\u0027].values).most_common()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nset(no_genes_vars).intersection(set(ju))"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nno_genes.show(1)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nj \u003d j.drop_duplicates([\u0027unique_variant_id\u0027])"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ncols \u003d [ \u0027chromosome\u0027,\u0027start\u0027, \u0027end\u0027, \u0027reference\u0027, \u0027alternate\u0027, \u0027unique_variant_id\u0027, \u0027qual\u0027,\n             \u0027INFO_DP\u0027, \u0027splitFromMultiAllelic\u0027, \u0027genotypes\u0027, \u0027symbol\u0027, \u0027rsID\u0027, \u0027cadd_score\u0027, \u0027eqtl_boolean\u0027]\n             \nchd_BS_scored_vars_mapped \u003d j.select(cols).union(has_genes.select(cols))\nchd_BS_scored_vars_mapped.show(1)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nchd_BS_scored_vars_wgenes  \u003d chd_BS_scored_vars_mapped.where(col(\u0027symbol\u0027).isNotNull())"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf_join \u003d chd_BS_scored_vars_wgenes\n\ndf_join \u003d df_join.withColumn(\u0027calls\u0027, df_join.genotypes.getItem(0).getItem(\u00276\u0027))\ndf_join \u003d df_join.withColumn(\u0027calls_str\u0027,concat(df_join.calls.getItem(0), lit(\":\"), df_join.calls.getItem(1) ))\ndf_join \u003d df_join.withColumn(\u0027AdditiveGenotype\u0027,when(df_join.calls_str.contains(\u0027-\u0027)  ,-9)\\\n                      .otherwise(df_join.calls.getItem(0) + df_join.calls.getItem(1)))\n\n# Create Participant ID column (should just be one participant per file)\ndf_join \u003d df_join.withColumn(\u0027participant_id\u0027, df_join.genotypes.getItem(0).getItem(\u00270\u0027))\ndf_join \u003d df_join.withColumn(\u0027INFO_DP\u0027,df_join.genotypes.getItem(0).getItem(\u002710\u0027))\ndf_join \u003d df_join.withColumn(\u0027qual\u0027,df_join.genotypes.getItem(0).getItem(\u00271\u0027))\ndf_join \u003d df_join.withColumn(\u0027PL\u0027,df_join.genotypes.getItem(0).getItem(\u00279\u0027))\n\ndf_join \u003d df_join.withColumn(\u0027AdditiveGenotype\u0027,when((df_join.INFO_DP \u003c 15) | (df_join.qual \u003c 25) ,-9)\\\n                      .otherwise(df_join.AdditiveGenotype))\ndf_join \u003d df_join.where(~ ((df_join.splitFromMultiAllelic \u003d\u003d \u0027true\u0027) \u0026 (df_join.alternate \u003d\u003d \u0027\u003cNON_REF\u003e\u0027) ))\n\n#df_join \u003d df_join.sort([\u0027chromosome\u0027,\u0027start\u0027])\n#df_join \u003d df_join.drop(*(\"splitFromMultiAllelic\")).dropDuplicates([\u0027unique_variant_id\u0027,\u0027AdditiveGenotype\u0027])\n\ndf_join \u003d df_join.withColumn(\u0027PL_00\u0027,df_join.PL.getItem(0)).withColumn(\u0027PL_01\u0027,df_join.PL.getItem(1)).withColumn(\u0027PL_11\u0027,df_join.PL.getItem(2))\n\ndf_join \u003d df_join.withColumn(\u0027AdditiveGenotype\u0027,when((df_join.PL_00 \u003d\u003d 0) \u0026 (df_join.PL_01 \u003d\u003d 0) \u0026\\\n                                           (col(\u0027AdditiveGenotype\u0027) \u003d\u003d 0 ),-9).otherwise(df_join.AdditiveGenotype))\n                                           \ndf_join \u003d df_join.withColumn(\u0027AdditiveGenotype\u0027,when(((df_join.PL_00 \u003d\u003d 0) \u0026 (df_join.PL_01 \u003d\u003d 0) \u0026\\\n                     (df_join.PL_11 \u003d\u003d 0)) \u0026 (col(\u0027AdditiveGenotype\u0027) \u003d\u003d 0 ) ,-9).otherwise(df_join.AdditiveGenotype))\n\ndf_join \u003d df_join.drop(*(\"PL\",\"PL_00\",\"PL_01\",\"PL_11\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nchd_BS_scored_vars_wgenes \u003d df_join.select(\u0027chromosome\u0027,\u0027start\u0027,\u0027end\u0027,\u0027reference\u0027,\u0027alternate\u0027,\n                                                \u0027unique_variant_id\u0027,\u0027symbol\u0027,\u0027rsID\u0027,\u0027cadd_score\u0027,\u0027eqtl_boolean\u0027,\u0027AdditiveGenotype\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nCounter(chd_BS_scored_vars_mapped.drop_duplicates([\u0027unique_variant_id\u0027]).select(\u0027symbol\u0027).toPandas()[\u0027symbol\u0027].values).most_common()"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# get the frequency of each gene\ndf \u003d chd_BS_scored_vars_wgenes.toPandas()\ndf[\u0027cadd_score\u0027] \u003d df[\u0027cadd_score\u0027].astype(np.float64)\n\nsymbol_freqs \u003d dict(Counter(df[\u0027symbol\u0027]))\nsymbol_freq_df \u003d pd.DataFrame.from_dict(symbol_freqs,orient\u003d\u0027index\u0027,columns\u003d[\u0027frequency\u0027]) \nsymbol_freq_df.index.name \u003d \u0027symbol\u0027\nsymbol_freq_df.head(3)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngene_scores \u003d df[[\u0027symbol\u0027,\u0027cadd_score\u0027]].groupby(\u0027symbol\u0027).sum()\ndf_genes \u003d gene_scores.join(symbol_freq_df).sort_values(\u0027cadd_score\u0027,ascending\u003dFalse)\ndf_genes[\u0027symbol\u0027] \u003d df_genes.index\ndf_genes \u003d df_genes.reset_index(drop\u003dTrue)\ndf_genes[\u0027cadd_score_gene_agg\u0027] \u003d df_genes[\u0027cadd_score\u0027]\ndf_genes.drop(\u0027cadd_score\u0027,axis\u003d1,inplace\u003dTrue)\ndf_genes.head(5)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# merge the variants back in\nrejoin \u003d df_genes.merge(df,on\u003d\u0027symbol\u0027,how\u003d\u0027right\u0027)\nprint(rejoin.shape)\nrejoin.head(1)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_genes \u003d rejoin"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngenome_bed \u003d spark.read.csv(\u0027s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/gene_regions_refseq\u0027,header\u003dTrue,sep\u003d\u0027\\t\u0027)\ngenome_bed \u003d genome_bed.withColumn(\u0027symbol\u0027,col(\u0027name2\u0027)).drop(\u0027name2\u0027)\ngenome_bed.show(1)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngenome_bed_select \u003d genome_bed.select([\u0027chrom\u0027,\u0027txStart\u0027,\u0027txEnd\u0027,\u0027cdsStart\u0027,\u0027cdsEnd\u0027,\u0027symbol\u0027])\nprint(genome_bed_select.count())\n\ngenome_bed_noDups \u003d genome_bed_select.dropDuplicates([\u0027symbol\u0027])\nprint(genome_bed_noDups.count())\n\ngenome_bed_noDups \u003d genome_bed_noDups.withColumn(\u0027txDiff\u0027,col(\u0027txEnd\u0027) - col(\u0027txStart\u0027))\ngenome_bed_noDups.show(3)"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# How much coverage do we have for our list of genes?\nbed_genes \u003d genome_bed_noDups.select(\u0027symbol\u0027).collect()\nmy_genes \u003d set(list(np.unique(df_genes[\u0027symbol\u0027]))) # 30988\nbed_genes \u003d set(list(np.unique([i[0] for i in bed_genes])))\nlen(bed_genes.intersection(my_genes))"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_Gene_spark \u003d spark.createDataFrame(df_genes)   # How did CADD_score get so many decimals?\n\nnormed \u003d genome_bed_noDups.select([\u0027symbol\u0027,\u0027txDiff\u0027]).join(df_Gene_spark,[\u0027symbol\u0027])\n\nnormed \u003d normed.withColumn(\u0027normed_CADD_score\u0027, F.round(1000*(col(\u0027cadd_score_gene_agg\u0027)/col(\u0027txDiff\u0027)),3 )).drop(\u0027hgvsc\u0027,\u0027hgvsg\u0027,\u0027strand\u0027)\n\nnormed.sort(\u0027normed_CADD_score\u0027,ascending\u003dFalse).show(50)"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nnormed.drop_duplicates([\u0027symbol\u0027]).sort(\u0027normed_CADD_score\u0027,ascending\u003dFalse).select(\u0027symbol\u0027,\u0027normed_CADD_score\u0027).show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#  Get rid of MicroRNA\u0027s (starts with MIR) microRNAs and Small nucleolar RNAs (snoRNAs)\nnormed_filter \u003d normed.where(~ (col(\u0027symbol\u0027).startswith(\u0027MIR\u0027)) | ((col(\u0027symbol\u0027).startswith(\u0027SNO\u0027))) )"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nnormed_filter \u003d normed_filter.withColumn(\u0027cadd_score_gene_agg\u0027,F.round(col(\u0027cadd_score_gene_agg\u0027),3)).drop(\u0027reference\u0027,\u0027alternate\u0027,\u0027start\u0027,\u0027txDiff\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n.repartitionByRange(10, \"chromosome\", \"start\").write\\\n      .mode(\u0027overwrite\u0027)\\\n      .partitionBy(\"chromosome\")\\\n       .parquet(\"tcsq_variant_gene_mappings.parquet\")"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nnormed_filter.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsnps \u003d spark.read.csv(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/results_chd_cadd/snp_cadd_gt20.csv\")\nsnps \u003d snps.select(col(\"_c0\").alias(\"chromosome\"), col(\"_c1\").alias(\"start\"),\ncol(\"_c2\").alias(\"reference\"),col(\"_c3\").alias(\"alternate\"),col(\u0027_c4\u0027).alias(\u0027CADD_score\u0027))\nprint(snps.count())\nsnps.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# read in indel variants that have been merged w/ CADD scores (gt 20)\nnonsnps \u003d spark.read.csv(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/results_chd_cadd/NON_snp_cadd_gt20.csv\")\nprint(nonsnps.count())\n\nnonsnps \u003d nonsnps.select(col(\"_c0\").alias(\"chromosome\"), col(\"_c1\").alias(\"start\"),\n                        col(\"_c2\").alias(\"reference\"),col(\"_c3\").alias(\"alternate\"),col(\u0027_c4\u0027).alias(\u0027CADD_score\u0027))\n#nonsnps.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# concatenate snps and non-snps results\nvariants_cadd \u003d snps.union(nonsnps)\n\nprint(variants_cadd.count())\n#variants_cadd.show(3)"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\u0027\u0027\u0027study_id_list \u003d [\u0027SD_PREASA7S\u0027]\nt_ocr \u003d spark \\\n    .table(\u0027occurrences\u0027) \\\n    .where((F.col(\u0027study_id\u0027).isin(study_id_list)) \u0026  (F.col(\u0027is_proband\u0027) \u003d\u003d \u0027true\u0027 ) )\n                     \nt_ocr.count()\u0027\u0027\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# FOR SAVING THE GENE-VARIANT MAPPINGS BY CHROM SO I CAN SAVE THEM AS CSV AND TRANSFER TO CAVATICA\nchroms \u003d [str(c) for c in list(range(4,23))] + [\u0027X\u0027,\u0027Y\u0027]\n\nfor CHROM in chroms:\n    print(CHROM)\n    t_csq_select_chr_pandas \u003d t_csq_select.where(col(\u0027chromosome\u0027).isin([CHROM]))\n    t_csq_select_chr_pandas.toPandas().to_csv(f\u0027tcsq_variant_gene_mappings_july27_chrom{CHROM}.csv\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\ncp tcsq_variant_gene_mappings_july27_chromY.csv ~/cavatica/projects/taylordm/taylor-urbs-r03-kf-cardiac/\n"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nt_csq_select_chr1to3 \u003d t_csq_select.where(col(\u0027chromosome\u0027).isin([\u00271\u0027,\u00272\u0027,\u00273\u0027]))\n\nprint(t_csq_select_chr1to3.count())\n#.write.parquet(\n#          \"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/tcsq_variant_gene_mappings_july25_chroms1to6.parquet\")"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nj \u003d spark.read.parquet(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/tcsq_variant_gene_mappings_july13.parquet\")\nprint(j.count())\nj.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nt_csq_select.repartitionByRange(60, \"chromosome\", \"start\").write\\\n      .mode(\u0027overwrite\u0027)\\\n      .partitionBy(\"chromosome\")\\\n       .parquet(\"tcsq_variant_gene_mappings.parquet\")\n      #.parquet(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/cadd_scores/tcsq_variant_gene_mappings.parquet\")"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nt_csq_select_noDups \u003d t_csq_select.dropDuplicates([\u0027chromosome\u0027,\u0027start\u0027, \u0027reference\u0027,\u0027alternate\u0027])\n\n# whats the diifference in these dups?"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nvars_csq_merge \u003d variants_cadd.join(t_csq_select_noDups,[\u0027chromosome\u0027,\u0027start\u0027, \u0027reference\u0027,\u0027alternate\u0027],\u0027left\u0027)\n\nvars_csq_merge \u003d vars_csq_merge.withColumn(\u0027unique_variant_id\u0027,concat(col(\"chromosome\"), lit(\":\"),col(\"start\"),lit(\u0027:\u0027),col(\u0027reference\u0027), lit(\u0027:\u0027),col(\u0027alternate\u0027),lit(\u0027:\u0027))) \n\nvars_csq_merge.count() # 1166795 w/ snps only, 247420 w/ dups in t_csq_select, 171008 w/o dups"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nvars_csq_merge \u003d vars_csq_merge.dropDuplicates() # what are these duplicates? see jeremys note\n#vars_csq_merge.where(col(\u0027reference\u0027) \u003d\u003d \u0027ACCACCCCAGAGCCCAC\u0027).show() # why are there doubles still (after dropping doubles in vars_csq_merge )? \n#d \u003d vars_csq_merge.groupBy(vars_csq_merge.columns).count().filter(\"count \u003e 1\")\n#d.drop(\u0027count\u0027).show() \nprint(vars_csq_merge.count())"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_snps \u003d vars_csq_merge.toPandas()"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#df_snps#.duplicated()\n\nCounter(df_snps[\u0027impact\u0027])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Take HIGH impact variations from our results df and compare with the highImpacts file to find which HIGH impact variations we did NOT pick up with the CADD \u003e 20 filtering. We want to find the CADD scores for these variants even if they are not greater than 20. \n"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\u0027\u0027\u0027# Load the huge CADD file (80GB) and remove first 2 lines, they are headers.\nwhole_genome_cadd \u003d spark.read.csv(\u0027s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/whole_genome_SNVs.tsv.gz\u0027,\n                    header\u003dFalse,sep\u003d\u0027 \u0027)\nwhole_genome_cadd \u003d whole_genome_cadd.select([\u0027_c0\u0027]).where(~col(\u0027_c0\u0027).startswith(\u0027#\u0027))\nwhole_genome_cadd \u003d whole_genome_cadd.withColumn(\u0027chromosome\u0027, F.split(whole_genome_cadd[\u0027_c0\u0027], \u0027\\t\u0027).getItem(0)) \\\n                               .withColumn(\u0027start\u0027, F.split(whole_genome_cadd[\u0027_c0\u0027], \u0027\\t\u0027).getItem(1)) \\\n                               .withColumn(\u0027reference\u0027, F.split(whole_genome_cadd[\u0027_c0\u0027], \u0027\\t\u0027).getItem(2)) \\\n                                .withColumn(\u0027alternate\u0027, F.split(whole_genome_cadd[\u0027_c0\u0027], \u0027\\t\u0027).getItem(3)) \\\n                                .withColumn(\u0027raw\u0027, F.split(whole_genome_cadd[\u0027_c0\u0027], \u0027\\t\u0027).getItem(4)) \\\n                                 .withColumn(\u0027PHRED\u0027, F.split(whole_genome_cadd[\u0027_c0\u0027], \u0027\\t\u0027).getItem(5)).drop(\u0027_c0\u0027)\u0027\u0027\u0027                                 \n                                 \n#highImpact \u003d spark.read.parquet(\u0027s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/highImpact_noDups\u0027)\nhighImpact_all \u003d t_csq_select.where(col(\u0027impact\u0027)\u003d\u003d\u0027HIGH\u0027)\n\nhighImpact_CADD \u003d vars_csq_merge.where(col(\u0027impact\u0027)\u003d\u003d\u0027HIGH\u0027)\nprint(highImpact_all.count())\nprint(highImpact_CADD.count())\n\nhighImpact_all \u003d highImpact_all.withColumn(\u0027unique_variant_id\u0027,concat(col(\"chromosome\"), lit(\":\"),col(\"start\"),lit(\u0027:\u0027),col(\u0027reference\u0027), lit(\u0027:\u0027),col(\u0027alternate\u0027),lit(\u0027:\u0027)))\nhighImpact_CADD \u003d highImpact_CADD.withColumn(\u0027unique_variant_id\u0027,concat(col(\"chromosome\"), lit(\":\"),col(\"start\"),lit(\u0027:\u0027),col(\u0027reference\u0027), lit(\u0027:\u0027),col(\u0027alternate\u0027),lit(\u0027:\u0027))) "
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nhighImpact_sub \u003d highImpact_all.select(\u0027unique_variant_id\u0027).subtract(highImpact_CADD.select(\u0027unique_variant_id\u0027))\nhighImpact_sub.count() # 10726 , do we get variants w/ cadd score gt20 with this method?\n# should be 6899"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nhighImpact_sub.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nhighImpact_sub_2 \u003d highImpact_sub.join(vars_csq_merge,\u0027unique_variant_id\u0027,\u0027inner\u0027)\n\nprint(highImpact_sub_2.count())\nhighImpact_sub_2.columns"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nhighImpact_sub_2.show(1)"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#vars_csq_merge.select(\u0027symbol\u0027).where(col(\u0027symbol\u0027).isNull()).count()\n#highImpact_sub.dropDuplicates([\u0027chromosome\u0027,\u0027start\u0027,\u0027reference\u0027,\u0027alternate\u0027]).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nhighImpact_indels \u003d highImpact_sub_2.where(  (F.length(\"reference\") \u003e 1) | (F.length(\"alternate\") \u003e 1)  )\nhighImpact_SNPs \u003d highImpact_sub_2.where(  ~( (F.length(\"reference\") \u003e 1) | (F.length(\"alternate\") \u003e 1) ) )\n\nprint(highImpact_indels.count())\nprint(highImpact_SNPs.count())"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nchroms \u003d [\u00271\u0027, \u002710\u0027, \u002711\u0027, \u002712\u0027, \u002713\u0027, \u002714\u0027, \u002715\u0027, \u002716\u0027, \u002717\u0027, \u002718\u0027, \u002719\u0027, \u00272\u0027, \u002720\u0027, \u002722\u0027, \u00273\u0027, \u00274\u0027, \u00275\u0027, \u00276\u0027, \u00277\u0027,\u00278\u0027, \u00279\u0027, \u0027X\u0027,\u0027Y\u0027]\n\nfor CHROM in chroms:\n    cadd_chrom \u003d spark.read.parquet(f\u0027s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/cadd_chrom_files/chr{CHROM}\u0027)\n\n    cadd_chrom \u003d cadd_chrom.select(col(\"0\").alias(\"chromosome\"), col(\"1\").alias(\"start\"),\n                    col(\"2\").alias(\"reference\"),col(\"3\").alias(\"alternate\"),col(\u00275\u0027).alias(\u0027CADD_score\u0027))\n    highImpact_SNPs_chrom \u003d highImpact_SNPs.where(col(\u0027chromosome\u0027) \u003d\u003d CHROM)\n    \n    print(f\u0027Joining CADD scores for chromosome {CHROM} ({highImpact_SNPs_chrom.count()} x {cadd_chrom.count()})...\u0027,end\u003d\u0027\u0027)\n    highImpact_results_chrom \u003d highImpact_SNPs_chrom.join(cadd_chrom, [\"chromosome\",\"start\",\"reference\", \"alternate\"], \"inner\")\n    print(f\u0027Done...Result is {highImpact_results_chrom.count()} rows long.\u0027)\n    \n    if CHROM \u003d\u003d \u00271\u0027: highImpact_SNPs_full \u003d  highImpact_results_chrom\n    else: highImpact_SNPs_full \u003d highImpact_SNPs_full.union(highImpact_results_chrom)\n    #if CHROM \u003d\u003d \u002712\u0027: break"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nhighImpact_SNPs_full.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nhighImpact_results_chrom.where(col(\u0027CADD_score\u0027).isNull()).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\n# this file, cadd_gnomad_parquet contains only scores gt 20, NOT all cadd scores which is what we want here.\ncadd_gnomad_chrom \u003d spark.read.parquet(\u0027s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/cadd_scores/cadd_gnomad_parquet/\u0027)\n\nfor CHROM in chroms:\n\n    cadd_chrom \u003d  cadd_gnomad_chrom.where(col(\u0027chromosome\u0027) \u003d\u003d CHROM)\n    highImpact_indels_chrom \u003d highImpact_indels.where(col(\u0027chromosome\u0027) \u003d\u003d CHROM)\n    \n\n    print(f\u0027Joining CADD scores for chromosome {CHROM} ({cadd_chrom.count()} x {highImpact_indels_chrom.count()})...\u0027,end\u003d\u0027\u0027)\n    \n    highImpact_results_chrom \u003d highImpact_indels_chrom.join(cadd_chrom, [\"chromosome\",\"start\",\"reference\", \"alternate\"], \"inner\")\n    \n    print(f\u0027Done...Result is {highImpact_results_chrom.count()} rows long.\u0027)\n    \n    if CHROM \u003d\u003d \u00271\u0027: highImpact_indels_full \u003d  highImpact_results_chrom\n    else: highImpact_indels_full \u003d highImpact_indels_full.union(highImpact_results_chrom)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nhighImpact_indels_full.count() "
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nhighImpact_MERGED \u003d  highImpact_SNPs_full.union(highImpact_indels_full)  \nhighImpact_MERGED.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nhighImpact_MERGED.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# no strand column\nhighImpact_MERGED \u003d highImpact_MERGED.drop(\u0027CADD_phred\u0027,\u0027CADD_raw\u0027,\u0027CADD_raw_rankscore\u0027,\u0027GTEx_V8_gene\u0027,\u0027GTEx_V8_tissue\u0027,\u0027end\u0027,\u0027hgvsp\u0027,\u0027ensembl_gene_id\u0027)#.show(2)\nhighImpact_MERGED \u003d highImpact_MERGED.withColumn(\u0027strand\u0027, lit(None).cast(StringType()))"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nordered_cols \u003d [\u0027chromosome\u0027, \u0027start\u0027, \u0027reference\u0027, \u0027alternate\u0027, \u0027CADD_score\u0027, \u0027rsID\u0027, \u0027impact\u0027,\n                 \u0027symbol\u0027, \u0027biotype\u0027, \u0027hgvsc\u0027, \u0027hgvsg\u0027,\u0027feature_type\u0027, \u0027strand\u0027, \u0027consequences\u0027, \u0027unique_variant_id\u0027]\n                 \ndf_vars \u003d vars_csq_merge.select(ordered_cols).union(highImpact_MERGED.select(ordered_cols))\ndf_vars.count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Now find CADD scores for eQTLs\n"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\neqtls \u003d spark.read.csv(\u0027s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/eqtls_gtexV8.csv\u0027,header\u003dTrue)\n\n\neqtls_select \u003d eqtls.select(col(\"chr\").alias(\"chromosome\"), col(\"variant_pos\").alias(\"start\"),\n                             col(\"ref\").alias(\"reference\"),col(\"alt\").alias(\"alternate\"),col(\u0027gene_name\u0027),col(\u0027rs_id_dbSNP151_GRCh38p7\u0027))\n                             \neqtls_select \u003d eqtls_select.withColumn(\u0027chromosome\u0027,F.regexp_replace(col(\"chromosome\"), \"[chr,]\", \"\") )\n\neqtls_indels \u003d eqtls_select.where(  (F.length(\"reference\") \u003e 1) | (F.length(\"alternate\") \u003e 1)  )\neqtls_SNPs \u003d eqtls_select.where(  ~( (F.length(\"reference\") \u003e 1) | (F.length(\"alternate\") \u003e 1) ) )\n\n#eqtls_select.show(3)"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\neqtls_indels.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfor CHROM in chroms:\n    cadd_chrom \u003d spark.read.parquet(f\u0027s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/cadd_chrom_files/chr{CHROM}\u0027)\n\n    cadd_chrom \u003d cadd_chrom.select(col(\"0\").alias(\"chromosome\"), col(\"1\").alias(\"start\"),\n                    col(\"2\").alias(\"reference\"),col(\"3\").alias(\"alternate\"),col(\u00275\u0027).alias(\u0027CADD_score\u0027))\n         \n    eqtls_SNPs_chrom \u003d eqtls_SNPs.where(col(\u0027chromosome\u0027) \u003d\u003d CHROM)\n    \n    print(f\u0027Joining CADD scores for chromosome {CHROM} ({eqtls_SNPs_chrom.count()} x {cadd_chrom.count()})...\u0027,end\u003d\u0027\u0027)\n    \n    eqtls_results_chrom \u003d eqtls_SNPs_chrom.join(cadd_chrom, [\"chromosome\",\"start\",\"reference\", \"alternate\"], \"inner\")\n    \n    print(f\u0027Done...Result is {eqtls_results_chrom.count()} rows long.\u0027)\n    \n    \n    if CHROM \u003d\u003d \u00271\u0027: eqtls_SNPs_full \u003d  eqtls_results_chrom\n    else: eqtls_SNPs_full \u003d eqtls_SNPs_full.union(eqtls_results_chrom)\n          \n    #if CHROM \u003d\u003d \u002712\u0027: break\n"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\neqtl_MERGED \u003d eqtls_SNPs_full.union(eqtls_indels_full)\neqtl_MERGED.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\neqtl_MERGED.show(1)"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\neqtl.show(1)"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#df_snps \u003d df_vars.toPandas()\n\ndf_snps_noGene  \u003d df_snps[df_snps.symbol.isnull()]\ndf_snps_Gene  \u003d df_snps[~df_snps.symbol.isnull()]"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\u0027\u0027\u0027\nSNPnexus_df \u003d df_snps_noGene[[\u0027chromosome\u0027,\u0027start\u0027,\u0027reference\u0027,\u0027alternate\u0027,\u0027strand\u0027]] \n#SNPnexus_df.columns \u003d [\u0027#CHROM\u0027,\u0027POS\u0027,\u0027ID\u0027,\u0027REF\u0027,\u0027ALT\u0027]\n#SNPnexus_df[\u0027ID\u0027] \u003d \u0027.\u0027\n#SNPnexus_df.    #fix dels \nSNPnexus_df[\u0027strand\u0027] \u003d 1\n\nSNPnexus_df[\u0027chromosome_col\u0027] \u003d \u0027Chromosome\u0027\nSNPnexus_df \u003d SNPnexus_df[[\u0027chromosome_col\u0027,\u0027chromosome\u0027,\u0027start\u0027,\u0027reference\u0027,\u0027alternate\u0027,\u0027strand\u0027]]\nassert SNPnexus_df.isna().sum().sum() \u003d\u003d 0\n\nSNPnexus_df.to_csv(\u0027/home/notebook/cavatica/projects/taylordm/taylor-urbs-r03-kf-cardiac/SNPnexus_df.csv\u0027,index\u003dFalse,sep\u003d\u0027 \u0027,header\u003dFalse)\u0027\u0027\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\u0027\u0027\u0027SNPnexus_RESULTS \u003d pd.read_csv(\u0027/home/notebook/cavatica/projects/taylordm/taylor-urbs-r03-kf-cardiac/near_gens_edc7a0e5.txt\u0027,sep\u003d\u0027\\t\u0027)\n\nSNPnexus_RESULTS[\u0027chromosome\u0027] \u003d  [i.replace(\u0027chr\u0027,\u0027\u0027) for i in SNPnexus_RESULTS[\u0027Chromosome\u0027]]\nSNPnexus_RESULTS[\u0027start\u0027] \u003d  SNPnexus_RESULTS[\u0027Position\u0027]\nSNPnexus_RESULTS[\u0027reference\u0027] \u003d  [i.split(\u0027:\u0027)[2].split(\u0027/\u0027)[0] for i in SNPnexus_RESULTS[\u0027Variation ID\u0027]]\nSNPnexus_RESULTS[\u0027alternate\u0027] \u003d  [i.split(\u0027:\u0027)[2].split(\u0027/\u0027)[1] for i in SNPnexus_RESULTS[\u0027Variation ID\u0027]]\n\nSNPnexus_nearest \u003d SNPnexus_RESULTS[SNPnexus_RESULTS[\u0027Overlapped Gene\u0027] \u003d\u003d \u0027None\u0027]\nSNPnexus_genes \u003d SNPnexus_RESULTS[~(SNPnexus_RESULTS[\u0027Overlapped Gene\u0027] \u003d\u003d \u0027None\u0027)]\nSNPnexus_genes[\u0027symbol\u0027] \u003d SNPnexus_genes[\u0027Overlapped Gene\u0027]\n\nSNPnexus_genes.head(3)\u0027\u0027\u0027"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## How to add these back to the df_snps_noGene dataframe?\n"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# filter for genes that appear more than once (meaning the variant is overlapped by 2 genes)\nSNPnexus_multi \u003d SNPnexus_genes[SNPnexus_genes.groupby(\u0027Overlapped Gene\u0027)[\u0027Overlapped Gene\u0027].transform(\u0027size\u0027) \u003e 1]\n\n# pick closest gene for variants assigned to multiple genes.\n\n# filter for the other genes that just show up once. these can be added directly to the \nSNPnexus_single \u003d SNPnexus_genes[SNPnexus_genes.groupby(\u0027Overlapped Gene\u0027)[\u0027Overlapped Gene\u0027].transform(\u0027size\u0027) \u003d\u003d 1]"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nupstream_mask \u003d SNPnexus_nearest[\u0027Distance to Nearest Upstream Gene\u0027] \u003e SNPnexus_nearest[\u0027Distance to Nearest Downstream Gene\u0027]"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nSNPnexus_upstream \u003d SNPnexus_nearest[upstream_mask]   \nSNPnexus_downstream \u003d SNPnexus_nearest[~upstream_mask]\n\n# take the names of these in a loop? and then make it a series"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nnp.all(SNPnexus_downstream[\u0027Distance to Nearest Upstream Gene\u0027] \u003c SNPnexus_downstream[\u0027Distance to Nearest Downstream Gene\u0027])"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_snps_noGene.drop(\u0027symbol\u0027,axis\u003d1,inplace\u003dTrue) # we\u0027ll merge in a new symbol column, but we need to do it only for the correct rows"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nlen(np.unique(SNPnexus_multi[\u0027Overlapped Gene\u0027].values))"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nCounter(SNPnexus_multi[\u0027Overlapped Gene\u0027].values).most_common()"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(vars_csq_merge.select(\u0027symbol\u0027).where(col(\u0027symbol\u0027).isNull()).count())\nprint(vars_csq_merge.select(\u0027rsID\u0027).where(col(\u0027rsID\u0027).isNull()).count())"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nCounter(df_snps[\u0027symbol\u0027]).most_common(5)   # normalize by size of gene?"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_snps \u003d df_vars.toPandas()\ndf_snps[\u0027CADD_score\u0027] \u003d df_snps[\u0027CADD_score\u0027].astype(np.float64)"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsymbol_freqs \u003d dict(Counter(df_snps[\u0027symbol\u0027]))\nsymbol_freq_df \u003d pd.DataFrame.from_dict(symbol_freqs,orient\u003d\u0027index\u0027,columns\u003d[\u0027frequency\u0027]) \nsymbol_freq_df.index.name \u003d \u0027symbol\u0027\nsymbol_freq_df.head(3)"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngene_scores \u003d df_snps[[\u0027symbol\u0027,\u0027CADD_score\u0027]].groupby(\u0027symbol\u0027).sum()"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\ndf_genes \u003d gene_scores.join(symbol_freq_df).sort_values(\u0027CADD_score\u0027,ascending\u003dFalse)\ndf_genes[\u0027symbol\u0027] \u003d df_genes.index\ndf_genes \u003d df_genes.reset_index(drop\u003dTrue)\n\n# remove micro rna\u0027s\n"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_genes # many rows in df_genes should get mapped multiple times to the main df., unless tthe genes frequency is 1 or 0.\n# do we need to merge this gene data with the variant level stuff?"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Normalize gene scores by dividing by length of gene\n"
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\naws s3 cp   ~/cavatica/projects/taylordm/taylor-urbs-r03-kf-cardiac/gene_regions_refseq  s3://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/gene_regions_refseq"
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngenome_bed \u003d spark.read.csv(\u0027s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/gene_regions_refseq\u0027,header\u003dTrue,sep\u003d\u0027\\t\u0027)\n\ngenome_bed \u003d genome_bed.withColumn(\u0027symbol\u0027,col(\u0027name2\u0027)).drop(\u0027name2\u0027)\ngenome_bed.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ngenome_bed_select \u003d genome_bed.select([\u0027chrom\u0027,\u0027txStart\u0027,\u0027txEnd\u0027,\u0027cdsStart\u0027,\u0027cdsEnd\u0027,\u0027symbol\u0027])\nprint(genome_bed_select.count())\n\ngenome_bed_noDups \u003d genome_bed_select.dropDuplicates([\u0027symbol\u0027])\nprint(genome_bed_noDups.count())\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ngenome_bed_noDups \u003d genome_bed_noDups.withColumn(\u0027txDiff\u0027,col(\u0027txEnd\u0027) - col(\u0027txStart\u0027))\n\ngenome_bed_noDups.show(3)"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nbed_genes \u003d genome_bed_noDups.select(\u0027symbol\u0027).collect()\n\nmy_genes \u003d set(list(np.unique(df_genes[\u0027symbol\u0027]))) # 30988\nbed_genes \u003d set(list(np.unique([i[0] for i in bed_genes])))\n\nlen(bed_genes.intersection(my_genes))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_snps_Gene_spark \u003d spark.createDataFrame(df_genes)   # How did CADD_score get so many decimals?\ndf_snps_Gene_spark.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nnormed \u003d genome_bed_noDups.select([\u0027symbol\u0027,\u0027txDiff\u0027]).join(df_snps_Gene_spark,[\u0027symbol\u0027])\n\nnormed \u003d normed.withColumn(\u0027normed_CADD_score\u0027, F.round(1000*(col(\u0027CADD_score\u0027)/col(\u0027txDiff\u0027)),3 )).drop(\u0027hgvsc\u0027,\u0027hgvsg\u0027,\u0027strand\u0027)\n\nnormed.sort(\u0027normed_CADD_score\u0027,ascending\u003dFalse).show(50)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### may need to use mean/median CADD score\n"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n # as expected, we lose about a third of the rows. we only have about 2/3 coverage on\n#\u0027symbol\u0027 with this BED file\nprint(df_snps_Gene_spark.count())\nprint(normed.count())"
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nplt.figure(figsize\u003d(10,10))\nplt.plot(df_genes.frequency,df_genes.CADD_score,\u0027.-\u0027,color\u003d\u0027r\u0027,linewidth\u003d\u00271\u0027)\nplt.ylim([0,6000])\nplt.xlim([0,300])\nz.show(plt, width\u003d\u0027700px\u0027)\nplt.close()"
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nwget https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_40/gencode.v40.annotation.gtf.gz\n"
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngencode \u003d pd.read_csv(\u0027gencode.v40.annotation.gtf.gz\u0027,skiprows\u003d5,sep\u003d\u0027\\t\u0027,header\u003dNone)\n\ngencode.columns \u003d [\u0027chromosome\u0027,\u0027source\u0027,\u0027type\u0027,\u0027start\u0027,\u0027end\u0027,\u00275\u0027,\u0027strand\u0027,\u00277\u0027,\u0027INFO\u0027]\n\ngencode_genes \u003d gencode[gencode[\u0027type\u0027] \u003d\u003d \u0027gene\u0027]\nprint(gencode_genes.shape)\n\n#Counter([len(k) for k in [i[4].split(\u0027 \u0027) for i in gencode_genes[\u0027INFO\u0027].str.split(\u0027;\u0027)]])\n#gencode_genes[\u0027hgnc_id\u0027] \u003d\ngencode_genes[\u0027gene_type\u0027] \u003d [i[1].split(\u0027 \u0027)[2][1:-1] for i in gencode_genes[\u0027INFO\u0027].str.split(\u0027;\u0027)]\ngencode_genes[\u0027symbol\u0027] \u003d [i[2].split(\u0027 \u0027)[2][1:-1] for i in gencode_genes[\u0027INFO\u0027].str.split(\u0027;\u0027)]\ngencode_genes[\u0027gene_id\u0027] \u003d [i[0].split(\u0027 \u0027)[1][1:-1] for i in gencode_genes[\u0027INFO\u0027].str.split(\u0027;\u0027)]\n\n\n#Counter(gencode_genes[\u0027gene_type\u0027]).most_common()\ngencode_genes_protein_coding \u003d gencode_genes[gencode_genes[\u0027gene_type\u0027] \u003d\u003d \u0027protein_coding\u0027]\n\ngencode_genes_protein_coding \u003d gencode_genes_protein_coding[gencode_genes_protein_coding[\u0027chromosome\u0027] !\u003d \u0027chrM\u0027]\n\nbed \u003d gencode_genes_protein_coding[[\u0027chromosome\u0027,\u0027start\u0027,\u0027end\u0027,\u0027strand\u0027,\u0027symbol\u0027,\u0027gene_id\u0027]].reset_index(drop\u003dTrue)\n\nbed_pos \u003d bed[bed[\u0027strand\u0027] \u003d\u003d \u0027+\u0027]\nbed_neg \u003d bed[bed[\u0027strand\u0027] \u003d\u003d \u0027-\u0027]\n\nbed_pos[\u0027start_upstream15kb\u0027] \u003d bed_pos[\u0027start\u0027] - 15000\nbed_pos[\u0027start_downstream15kb\u0027] \u003d bed_pos[\u0027end\u0027] + 5000\n\nbed_neg[\u0027start_upstream15kb\u0027] \u003d bed_neg[\u0027start\u0027] - 5000\nbed_neg[\u0027start_downstream15kb\u0027] \u003d bed_neg[\u0027end\u0027] + 15000\n\nbed_buffer \u003d pd.concat([bed_pos,bed_neg]).sort_values([\u0027chromosome\u0027,\u0027start\u0027])"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nbed_buffer_spark \u003d spark.createDataFrame(bed_buffer)\nbed_buffer_spark.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbed_buffer_spark.repartitionByRange(30, \"chromosome\", \"start\").write\\\n      .mode(\u0027overwrite\u0027)\\\n      .partitionBy(\"chromosome\")\\\n      .parquet(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/bed_protein_coding.parquet\")"
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\naws s3 ls s3://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/bed_protein_coding.parquet/\n"
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\naws s3 cp s3://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/bed_protein_coding.parquet  ~/cavatica/projects/taylordm/taylor-urbs-r03-kf-cardiac/bed_protein_coding.parquet --recursive"
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\u0027\u0027\u0027vars_csq_merge_HIGH \u003d vars_csq_merge.where(col(\u0027impact\u0027) \u003d\u003d\u0027HIGH\u0027)\nprint(vars_csq_merge_HIGH.count())\n\ndiscovered_HIGH \u003d vars_csq_merge_HIGH.select(\u0027unique_variant_id\u0027).collect()\ndiscovered_HIGH \u003d set(list(np.unique([i[0] for i in discovered_HIGH])))\n\nUndiscovered_HIGH \u003d highImpact.select(\u0027unique_variant_id\u0027).collect()\nUndiscovered_HIGH \u003d set(list(np.unique([i[0] for i in Undiscovered_HIGH])))\n\nprint(len(Undiscovered_HIGH))\nprint(len(discovered_HIGH))\n\nprint(len(Undiscovered_HIGH.difference(discovered_HIGH)))\ndiscovered_HIGH.difference(Undiscovered_HIGH)\nhighImpact_sub \u003d highImpact.where(~(F.col(\u0027unique_variant_id\u0027)).isin(discovered_HIGH))\nhighImpact_sub.count()\u0027\u0027\u0027\n"
    }
  ]
}