{
  "metadata": {
    "name": "cadd_joins",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pandas as pd\npd.set_option(\u0027display.max_columns\u0027, None)\nimport numpy as np\nimport glob\nimport io\nimport requests\nimport os\nfrom pyspark import SparkFiles\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col,collect_list, concat, lit, when, monotonically_increasing_id, concat_ws \nfrom pyspark.sql.functions import regexp_replace"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\naws s3 cp   ~/cavatica/projects/taylordm/taylor-urbs-r03-kf-cardiac/cadd_chrom_files_parquet/  s3://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/cadd_scores/cadd_chrom_files_parquet/ --recursive\n\n#aws s3 cp   ~/cavatica/projects/taylordm/taylor-urbs-r03-kf-cardiac/cadd_gnomad_gt_20.tsv  s3://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/cadd_scores/cadd_gnomad_gt_20.tsv\n#aws s3 cp   ~/cavatica/projects/taylordm/taylor-urbs-r03-kf-cardiac/cadd_chrom_files_gt20  s3://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/cadd_chrom_files/cadd_chrom_files_gt20 --recursive\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nCHD_filt_select.write.csv(f\u0027~/cavatica/projects/{project}/{file}\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\naws s3 rm --recursive s3://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/cadd_scores/cadd_parquet_ALL/\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\naws s3 ls  s3://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/cadd_scores/cadd_chrom_files_parquet/\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nchroms \u003d [\u00271\u0027, \u002710\u0027, \u002711\u0027, \u002712\u0027, \u002713\u0027, \u002714\u0027, \u002715\u0027, \u002716\u0027, \u002717\u0027, \u002718\u0027, \u002719\u0027, \u00272\u0027, \u002720\u0027, \u002722\u0027, \u00273\u0027, \u00274\u0027, \u00275\u0027, \u00276\u0027, \u00277\u0027,\u00278\u0027, \u00279\u0027, \u0027X\u0027,\u0027Y\u0027]\n\nfor CHROM in chroms:\n    cadd_chrom \u003d spark.read.parquet(f\u0027s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/cadd_chrom_files/chr{CHROM}\u0027)\n\n    #cadd_chrom \u003d cadd_chrom.select(col(\"0\").alias(\"chromosome\"), col(\"1\").alias(\"start\"),\n    #                col(\"2\").alias(\"reference\"),col(\"3\").alias(\"alternate\"),col(\u00275\u0027).alias(\u0027CADD_score\u0027))\n\n    \n    if CHROM \u003d\u003d \u00271\u0027: cadd_chrom_MASTER \u003d  cadd_chrom\n    else: cadd_chrom_MASTER \u003d cadd_chrom_MASTER.union(cadd_chrom)\n          \n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\naws s3 cp  s3://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/highImpact_noDups.parquet /home/notebook/cavatica/projects/taylordm/taylor-urbs-r03-kf-cardiac/highImpact_noDups.parquet --recursive "
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\naws s3 rm --recursive s3://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/highImpact_noDups.parquet"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# This file comes from https://cadd.gs.washington.edu/download titled \u0027All possible SNVs of GRCh38/hg38\u0027. \n# It was downloaded in a data cruncher notebook using the command:  \n# !wget https://krishna.gs.washington.edu/download/CADD/v1.6/GRCh38/whole_genome_SNVs.tsv.gz\n# the file is 81GB and it takes a very very long time to download it using the upload files mechanism on cavatica.\n# after downloading, I filtered the file to include only indels w/ CADD scores \u003d\u003e20 with the AWK command:\n# !zcat whole_genome_SNVs.tsv.gz | awk -F\"\\t\" \u0027$6 \u003e 20 { print\u003ecadd_gt_20.tsv}\u0027\n# the file was also split into seperate chromosome files with the command:\n# !zcat /sbgenomics/project-files/whole_genome_SNVs.tsv.gz | awk \u0027BEGIN {FS\u003d\"\\t\"}; {print\u003e$1}\u0027\n# but that is not neccessary.\n\n#Load in all tsv cadd gt 20 files into a single dataframe\ncadd_gt20 \u003d spark.read.csv(\u0027s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/cadd_chrom_files/cadd_chrom_files_gt20/\u0027,header\u003dNone,sep\u003d\u0027\\t\u0027)\n\ncadd_gt20 \u003d cadd_gt20.select(col(\"_c0\").alias(\"chromosome\"), col(\"_c1\").alias(\"start\"),\n                             col(\"_c2\").alias(\"reference\"),col(\"_c3\").alias(\"alternate\"),col(\"_c5\").alias(\"CADD_score\"))\nprint(cadd_gt20.count())"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncadd_gt20.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Need to check what ref genome kf variants were called against. gnomad 3.0 is build 38\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# This file comes from https://cadd.gs.washington.edu/download titled \u0027All gnomad release 3.0 InDels to initiate a local setup\u0027. \n# It was downloaded in a data cruncher notebook using the command: \n# !wget https://krishna.gs.washington.edu/download/CADD/v1.6/GRCh38/gnomad.genomes.r3.0.indel.tsv.gz\n# after downloading, I filtered the file to include only indels w/ CADD scores \u003d\u003e20 with the AWK command:\n# !zcat gnomad.genomes.r3.0.indel.tsv.gz | awk -F\"\\t\" \u0027$6 \u003e 20 { print\u003e\"cadd_gnomad_gt_20.tsv\"}\u0027\n\ncadd_gnomad_gt20 \u003d spark.read.csv(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/cadd_scores/cadd_gnomad_gt_20.tsv\"                                                                                                        ,header\u003dTrue,sep\u003d\u0027\\t\u0027)\n\ncadd_gnomad_gt20_select \u003d cadd_gnomad_gt20.select(col(\"#Chrom\").alias(\"chromosome\"), col(\"Pos\").alias(\"start\"),\n                             col(\"Ref\").alias(\"reference\"),col(\"Alt\").alias(\"alternate\"),col(\"PHRED\").alias(\"CADD_score\"))\n                             \ncadd_gnomad_gt20_select.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncadd_gnomad_gt20_select.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Read in our variants. These are produced in the \u0027\u0027 zeppelin notebook\nparquetFile \u003d spark.read.parquet(\u0027s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/variants_for_cadd_may26_35mil\u0027)\nparquetFile.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nnon_snps \u003d parquetFile.where(  (F.length(\"reference\") \u003e 1) | (F.length(\"alternate\") \u003e 1)  )\nprint(non_snps.count())\nnon_snps.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(non_snps.where(col(\u0027alternate\u0027) \u003d\u003d \u0027*\u0027).count())"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# reformat the non-snps, need to replace the deletions (where alt \u003d \u0027*\u0027) w/ the first base of the reference sequence. this is how the\nnon_snps \u003d non_snps.withColumn(\u0027first_base_ref\u0027,col(\u0027reference\u0027).substr(1,1))\n\n# create a new column \u0027new_al\u0027, where if the alt column \u003d\u003d \u0027*\u0027, then replace it w/ the first_base of the reference column in the new_alt column, otherwise, just use value in the alt column\n\nnon_snps_fix \u003d non_snps.withColumn(\u0027new_alt\u0027,when(non_snps.alternate \u003d\u003d \u0027*\u0027  ,non_snps.first_base_ref)\\\n                      .otherwise(non_snps.alternate))\n                      \n# rename new_alt --\u003e alternate                      \nnon_snps_fix \u003d non_snps_fix.drop(\u0027alternate\u0027,\u0027first_base_ref\u0027).withColumnRenamed(\u0027new_alt\u0027,\u0027alternate\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nresults_nonsnps \u003d non_snps_fix.join(cadd_gnomad_gt20_select,[\"chromosome\",\"start\",\"reference\",\"alternate\"], \"left\")\nprint(results_nonsnps.count())\nresults_nonsnps.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nresults_nonsnps_nonNull \u003d results_nonsnps.where(col(\u0027CADD_score\u0027).isNotNull())\nresults_nonsnps_nonNull.count()\nnp.round(100*(9743/8281949),2)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nresults_nonsnps_nonNull.write.mode(\u0027overwrite\u0027).csv(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/results_chd_cadd/NON_snp_cadd_gt20.csv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncadd_scores_gt20 \u003d spark.read.parquet(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/cadd_scores/cadd_parquet/\")\n\nchd_variants_snps \u003d spark.read.parquet(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/tables/variants_snps_cadd_june3/\")\n\nresult_snps \u003d chd_variants_snps.join(cadd_scores_gt20, [\"chromosome\",\"start\",\"reference\", \"alternate\"], \"left\")\nprint(result_snps.count())\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nresult_snps.write.mode(\u0027overwrite\u0027).csv(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/results_chd_cadd/snp_cadd_gt20.csv\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}